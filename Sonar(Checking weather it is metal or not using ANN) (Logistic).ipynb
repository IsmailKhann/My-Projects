{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"D:\\\\programs\\\\Excel files\\\\Sonar.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       0\n",
       "V2       0\n",
       "V3       0\n",
       "V4       0\n",
       "V5       0\n",
       "        ..\n",
       "V57      0\n",
       "V58      0\n",
       "V59      0\n",
       "V60      0\n",
       "Class    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,data.columns!='Class'].values\n",
    "Y=data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(20,),alpha=1e-5,solver='sgd',activation='logistic',tol=1e-4,learning_rate_init=0.1,verbose=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69928551\n",
      "Iteration 2, loss = 0.69177064\n",
      "Iteration 3, loss = 0.68323177\n",
      "Iteration 4, loss = 0.67452310\n",
      "Iteration 5, loss = 0.66566214\n",
      "Iteration 6, loss = 0.65628561\n",
      "Iteration 7, loss = 0.64609831\n",
      "Iteration 8, loss = 0.63507743\n",
      "Iteration 9, loss = 0.62343031\n",
      "Iteration 10, loss = 0.61143840\n",
      "Iteration 11, loss = 0.59932304\n",
      "Iteration 12, loss = 0.58719789\n",
      "Iteration 13, loss = 0.57509729\n",
      "Iteration 14, loss = 0.56303302\n",
      "Iteration 15, loss = 0.55103673\n",
      "Iteration 16, loss = 0.53917112\n",
      "Iteration 17, loss = 0.52751670\n",
      "Iteration 18, loss = 0.51615020\n",
      "Iteration 19, loss = 0.50512829\n",
      "Iteration 20, loss = 0.49448172\n",
      "Iteration 21, loss = 0.48421839\n",
      "Iteration 22, loss = 0.47433074\n",
      "Iteration 23, loss = 0.46480333\n",
      "Iteration 24, loss = 0.45561868\n",
      "Iteration 25, loss = 0.44676081\n",
      "Iteration 26, loss = 0.43821707\n",
      "Iteration 27, loss = 0.42997860\n",
      "Iteration 28, loss = 0.42203993\n",
      "Iteration 29, loss = 0.41439780\n",
      "Iteration 30, loss = 0.40704939\n",
      "Iteration 31, loss = 0.39999046\n",
      "Iteration 32, loss = 0.39321373\n",
      "Iteration 33, loss = 0.38670800\n",
      "Iteration 34, loss = 0.38045816\n",
      "Iteration 35, loss = 0.37444618\n",
      "Iteration 36, loss = 0.36865278\n",
      "Iteration 37, loss = 0.36305945\n",
      "Iteration 38, loss = 0.35765044\n",
      "Iteration 39, loss = 0.35241423\n",
      "Iteration 40, loss = 0.34734421\n",
      "Iteration 41, loss = 0.34243846\n",
      "Iteration 42, loss = 0.33769875\n",
      "Iteration 43, loss = 0.33312887\n",
      "Iteration 44, loss = 0.32873272\n",
      "Iteration 45, loss = 0.32451245\n",
      "Iteration 46, loss = 0.32046707\n",
      "Iteration 47, loss = 0.31659150\n",
      "Iteration 48, loss = 0.31287643\n",
      "Iteration 49, loss = 0.30930872\n",
      "Iteration 50, loss = 0.30587241\n",
      "Iteration 51, loss = 0.30254992\n",
      "Iteration 52, loss = 0.29932344\n",
      "Iteration 53, loss = 0.29617617\n",
      "Iteration 54, loss = 0.29309326\n",
      "Iteration 55, loss = 0.29006242\n",
      "Iteration 56, loss = 0.28707422\n",
      "Iteration 57, loss = 0.28412199\n",
      "Iteration 58, loss = 0.28120153\n",
      "Iteration 59, loss = 0.27831071\n",
      "Iteration 60, loss = 0.27544902\n",
      "Iteration 61, loss = 0.27261710\n",
      "Iteration 62, loss = 0.26981638\n",
      "Iteration 63, loss = 0.26704876\n",
      "Iteration 64, loss = 0.26431636\n",
      "Iteration 65, loss = 0.26162127\n",
      "Iteration 66, loss = 0.25896541\n",
      "Iteration 67, loss = 0.25635031\n",
      "Iteration 68, loss = 0.25377701\n",
      "Iteration 69, loss = 0.25124591\n",
      "Iteration 70, loss = 0.24875678\n",
      "Iteration 71, loss = 0.24630866\n",
      "Iteration 72, loss = 0.24389994\n",
      "Iteration 73, loss = 0.24152844\n",
      "Iteration 74, loss = 0.23919152\n",
      "Iteration 75, loss = 0.23688628\n",
      "Iteration 76, loss = 0.23460974\n",
      "Iteration 77, loss = 0.23235902\n",
      "Iteration 78, loss = 0.23013160\n",
      "Iteration 79, loss = 0.22792535\n",
      "Iteration 80, loss = 0.22573872\n",
      "Iteration 81, loss = 0.22357070\n",
      "Iteration 82, loss = 0.22142082\n",
      "Iteration 83, loss = 0.21928906\n",
      "Iteration 84, loss = 0.21717573\n",
      "Iteration 85, loss = 0.21508138\n",
      "Iteration 86, loss = 0.21300665\n",
      "Iteration 87, loss = 0.21095221\n",
      "Iteration 88, loss = 0.20891860\n",
      "Iteration 89, loss = 0.20690628\n",
      "Iteration 90, loss = 0.20491551\n",
      "Iteration 91, loss = 0.20294638\n",
      "Iteration 92, loss = 0.20099882\n",
      "Iteration 93, loss = 0.19907255\n",
      "Iteration 94, loss = 0.19716719\n",
      "Iteration 95, loss = 0.19528220\n",
      "Iteration 96, loss = 0.19341694\n",
      "Iteration 97, loss = 0.19157070\n",
      "Iteration 98, loss = 0.18974275\n",
      "Iteration 99, loss = 0.18793233\n",
      "Iteration 100, loss = 0.18613870\n",
      "Iteration 101, loss = 0.18436116\n",
      "Iteration 102, loss = 0.18259908\n",
      "Iteration 103, loss = 0.18085192\n",
      "Iteration 104, loss = 0.17911920\n",
      "Iteration 105, loss = 0.17740052\n",
      "Iteration 106, loss = 0.17569560\n",
      "Iteration 107, loss = 0.17400419\n",
      "Iteration 108, loss = 0.17232614\n",
      "Iteration 109, loss = 0.17066133\n",
      "Iteration 110, loss = 0.16900970\n",
      "Iteration 111, loss = 0.16737122\n",
      "Iteration 112, loss = 0.16574589\n",
      "Iteration 113, loss = 0.16413372\n",
      "Iteration 114, loss = 0.16253472\n",
      "Iteration 115, loss = 0.16094892\n",
      "Iteration 116, loss = 0.15937634\n",
      "Iteration 117, loss = 0.15781699\n",
      "Iteration 118, loss = 0.15627089\n",
      "Iteration 119, loss = 0.15473803\n",
      "Iteration 120, loss = 0.15321841\n",
      "Iteration 121, loss = 0.15171201\n",
      "Iteration 122, loss = 0.15021883\n",
      "Iteration 123, loss = 0.14873883\n",
      "Iteration 124, loss = 0.14727200\n",
      "Iteration 125, loss = 0.14581830\n",
      "Iteration 126, loss = 0.14437772\n",
      "Iteration 127, loss = 0.14295021\n",
      "Iteration 128, loss = 0.14153576\n",
      "Iteration 129, loss = 0.14013434\n",
      "Iteration 130, loss = 0.13874591\n",
      "Iteration 131, loss = 0.13737046\n",
      "Iteration 132, loss = 0.13600793\n",
      "Iteration 133, loss = 0.13465832\n",
      "Iteration 134, loss = 0.13332156\n",
      "Iteration 135, loss = 0.13199764\n",
      "Iteration 136, loss = 0.13068650\n",
      "Iteration 137, loss = 0.12938810\n",
      "Iteration 138, loss = 0.12810238\n",
      "Iteration 139, loss = 0.12682930\n",
      "Iteration 140, loss = 0.12556879\n",
      "Iteration 141, loss = 0.12432079\n",
      "Iteration 142, loss = 0.12308524\n",
      "Iteration 143, loss = 0.12186206\n",
      "Iteration 144, loss = 0.12065120\n",
      "Iteration 145, loss = 0.11945256\n",
      "Iteration 146, loss = 0.11826609\n",
      "Iteration 147, loss = 0.11709170\n",
      "Iteration 148, loss = 0.11592932\n",
      "Iteration 149, loss = 0.11477888\n",
      "Iteration 150, loss = 0.11364031\n",
      "Iteration 151, loss = 0.11251352\n",
      "Iteration 152, loss = 0.11139845\n",
      "Iteration 153, loss = 0.11029502\n",
      "Iteration 154, loss = 0.10920318\n",
      "Iteration 155, loss = 0.10812284\n",
      "Iteration 156, loss = 0.10705394\n",
      "Iteration 157, loss = 0.10599642\n",
      "Iteration 158, loss = 0.10495021\n",
      "Iteration 159, loss = 0.10391525\n",
      "Iteration 160, loss = 0.10289147\n",
      "Iteration 161, loss = 0.10187881\n",
      "Iteration 162, loss = 0.10087720\n",
      "Iteration 163, loss = 0.09988659\n",
      "Iteration 164, loss = 0.09890690\n",
      "Iteration 165, loss = 0.09793808\n",
      "Iteration 166, loss = 0.09698005\n",
      "Iteration 167, loss = 0.09603275\n",
      "Iteration 168, loss = 0.09509611\n",
      "Iteration 169, loss = 0.09417006\n",
      "Iteration 170, loss = 0.09325453\n",
      "Iteration 171, loss = 0.09234945\n",
      "Iteration 172, loss = 0.09145473\n",
      "Iteration 173, loss = 0.09057032\n",
      "Iteration 174, loss = 0.08969611\n",
      "Iteration 175, loss = 0.08883205\n",
      "Iteration 176, loss = 0.08797804\n",
      "Iteration 177, loss = 0.08713401\n",
      "Iteration 178, loss = 0.08629986\n",
      "Iteration 179, loss = 0.08547552\n",
      "Iteration 180, loss = 0.08466090\n",
      "Iteration 181, loss = 0.08385590\n",
      "Iteration 182, loss = 0.08306044\n",
      "Iteration 183, loss = 0.08227443\n",
      "Iteration 184, loss = 0.08149778\n",
      "Iteration 185, loss = 0.08073039\n",
      "Iteration 186, loss = 0.07997217\n",
      "Iteration 187, loss = 0.07922303\n",
      "Iteration 188, loss = 0.07848287\n",
      "Iteration 189, loss = 0.07775160\n",
      "Iteration 190, loss = 0.07702912\n",
      "Iteration 191, loss = 0.07631534\n",
      "Iteration 192, loss = 0.07561016\n",
      "Iteration 193, loss = 0.07491349\n",
      "Iteration 194, loss = 0.07422522\n",
      "Iteration 195, loss = 0.07354527\n",
      "Iteration 196, loss = 0.07287354\n",
      "Iteration 197, loss = 0.07220993\n",
      "Iteration 198, loss = 0.07155434\n",
      "Iteration 199, loss = 0.07090669\n",
      "Iteration 200, loss = 0.07026687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shawaf Khan\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(20,), learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_fun=15000, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "pred=mlp.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1\n",
      " 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
      " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "pred1=mlp.predict(X_train)\n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1\n",
      " 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
      " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "\n",
      " [[77  0]\n",
      " [ 0 68]]\n",
      "\n",
      " 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n",
    "print(accuracy_score(Y_train,pred1)*100)\n",
    "print(\"\\n\",confusion_matrix(Y_train,pred1))\n",
    "print(\"\\n\",f1_score(Y_train,pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
